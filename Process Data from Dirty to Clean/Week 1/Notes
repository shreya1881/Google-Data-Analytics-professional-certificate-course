-> A strong analysis depends on the integrity of the data. If the data you're using is compromised in any way, your analysis won't be as strong as it should be.

-> Data integrity is the accuracy, completeness, consistency, and trustworthiness of data throughout its lifecycle.

-> When data integrity is low, it can cause anything from the loss of a single pixel in an image to an incorrect medical decision. In some cases, one missing piece 
   can make all of your data useless.

-> Data integrity can be compromised in lots of different ways. There's a chance data can be compromised every time it's replicated, transferred, or manipulated in 
   any way. 

-> Data replication is the process of storing data in multiple locations. If you're replicating data at different times in different places, there's a chance your 
   data will be out of sync. This data lacks integrity because different people might not be using the same data for their findings, which can cause inconsistencies.
   
-> There's also the issue of data transfer, which is the process of copying data from a storage device to memory, or from one computer to another. If your data 
   transfer is interrupted, you might end up with an incomplete data set, which might not be useful for your needs. 
   
-> The data manipulation process involves changing the data to make it more organized and easier to read. Data manipulation is meant to make the data analysis process 
   more efficient, but an error during the process can compromise the efficiency. Finally, data can also be compromised through human error, viruses, malware, hacking,
   and system failures, which can all lead to even more headaches.

-> 1. Data replication compromising data integrity: Continuing with the example, imagine you ask your international counterparts to verify dates and stick to one 
      format. One analyst copies a large dataset to check the dates. But because of memory issues, only part of the dataset is actually copied. The analyst would be
      verifying and standardizing incomplete data. That partial dataset would be certified as compliant but the full dataset would still contain dates that weren't
      verified. Two versions of a dataset can introduce inconsistent results. A final audit of results would be essential to reveal what happened and correct all
      dates. 

   2. Data transfer compromising data integrity: Another analyst checks the dates in a spreadsheet and chooses to import the validated and standardized data back to 
      the database. But suppose the date field from the spreadsheet was incorrectly classified as a text field during the data import (transfer) process. Now some of 
      the dates in the database are stored as text strings. At this point, the data needs to be cleaned to restore its integrity. 

   3. Data manipulation compromising data integrity: When checking dates, another analyst notices what appears to be a duplicate record in the database and removes 
      it. But it turns out that the analyst removed a unique record for a company’s subsidiary and not a duplicate record for the company. Your dataset is now missing 
      data and the data must be restored for completeness.

-> 1. Data type
      Values must be of a certain type: date, number, percentage, Boolean, etc.
      If the data type is a date, a single number like 30 would fail the constraint and be invalid.
   
   2. Data range
      Values must fall between predefined maximum and minimum values
      If the data range is 10-20, a value of 30 would fail the constraint and be invalid
    
   3. Mandatory
      Values can’t be left blank or empty
      If age is mandatory, that value must be filled in
   
   4. Unique
      Values can’t have a duplicate
      Two people can’t have the same mobile phone number within the same service area
      
   5. Regular expression (regex) patterns
      Values must match a prescribed pattern
      A phone number must match ###-###-#### (no other characters allowed)
      
   6. Cross-field validation
      Certain conditions for multiple fields must be satisfied
      Values are percentages and values from multiple fields must add up to 100%
      
   7. Primary-key
      (Databases only) value must be unique per column
      A database table can’t have two rows with the same primary key value. A primary key is an identifier in a database that references a column in which each
      value is unique. More information about primary and foreign keys is provided later in the program.
   
   8. Set-membership
      (Databases only) values for a column must come from a set of discrete values 
      Value for a column must be set to Yes, No, or Not Applicable
   
   9. Foreign-key
      (Databases only) values for a column must be unique values coming from a column in another table
      In a U.S. taxpayer database, the State column must be a valid state or territory with the set of acceptable values defined in a separate States table
   
   10. Accuracy
      The degree to which the data conforms to the actual entity being measured or described
      If values for zip codes are validated by street location, the accuracy of the data goes up.
      
   11. Completeness
      The degree to which the data contains all desired components or measures
      If data for personal profiles required hair and eye color, and both are collected, the data is complete.
      
   12. Consistency
      The degree to which the data is repeatable from different points of entry or collection
      If a customer has the same address in the sales and repair databases, the data is consistent.

-> Clean data + alignment to business objective = accurate conclusions
