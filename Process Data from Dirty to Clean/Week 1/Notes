-> A strong analysis depends on the integrity of the data. If the data you're using is compromised in any way, your analysis won't be as strong as it should be.

-> Data integrity is the accuracy, completeness, consistency, and trustworthiness of data throughout its lifecycle.

-> When data integrity is low, it can cause anything from the loss of a single pixel in an image to an incorrect medical decision. In some cases, one missing piece 
   can make all of your data useless.

-> Data integrity can be compromised in lots of different ways. There's a chance data can be compromised every time it's replicated, transferred, or manipulated in 
   any way. 

-> Data replication is the process of storing data in multiple locations. If you're replicating data at different times in different places, there's a chance your 
   data will be out of sync. This data lacks integrity because different people might not be using the same data for their findings, which can cause inconsistencies.
   
-> There's also the issue of data transfer, which is the process of copying data from a storage device to memory, or from one computer to another. If your data 
   transfer is interrupted, you might end up with an incomplete data set, which might not be useful for your needs. 
   
-> The data manipulation process involves changing the data to make it more organized and easier to read. Data manipulation is meant to make the data analysis process 
   more efficient, but an error during the process can compromise the efficiency. Finally, data can also be compromised through human error, viruses, malware, hacking,
   and system failures, which can all lead to even more headaches.

-> 1. Data replication compromising data integrity: Continuing with the example, imagine you ask your international counterparts to verify dates and stick to one 
      format. One analyst copies a large dataset to check the dates. But because of memory issues, only part of the dataset is actually copied. The analyst would be
      verifying and standardizing incomplete data. That partial dataset would be certified as compliant but the full dataset would still contain dates that weren't
      verified. Two versions of a dataset can introduce inconsistent results. A final audit of results would be essential to reveal what happened and correct all
      dates. 

   2. Data transfer compromising data integrity: Another analyst checks the dates in a spreadsheet and chooses to import the validated and standardized data back to 
      the database. But suppose the date field from the spreadsheet was incorrectly classified as a text field during the data import (transfer) process. Now some of 
      the dates in the database are stored as text strings. At this point, the data needs to be cleaned to restore its integrity. 

   3. Data manipulation compromising data integrity: When checking dates, another analyst notices what appears to be a duplicate record in the database and removes 
      it. But it turns out that the analyst removed a unique record for a company’s subsidiary and not a duplicate record for the company. Your dataset is now missing 
      data and the data must be restored for completeness.

-> 1. Data type
      Values must be of a certain type: date, number, percentage, Boolean, etc.
      If the data type is a date, a single number like 30 would fail the constraint and be invalid.
   
   2. Data range
      Values must fall between predefined maximum and minimum values
      If the data range is 10-20, a value of 30 would fail the constraint and be invalid
    
   3. Mandatory
      Values can’t be left blank or empty
      If age is mandatory, that value must be filled in
   
   4. Unique
      Values can’t have a duplicate
      Two people can’t have the same mobile phone number within the same service area
      
   5. Regular expression (regex) patterns
      Values must match a prescribed pattern
      A phone number must match ###-###-#### (no other characters allowed)
      
   6. Cross-field validation
      Certain conditions for multiple fields must be satisfied
      Values are percentages and values from multiple fields must add up to 100%
      
   7. Primary-key
      (Databases only) value must be unique per column
      A database table can’t have two rows with the same primary key value. A primary key is an identifier in a database that references a column in which each
      value is unique. More information about primary and foreign keys is provided later in the program.
   
   8. Set-membership
      (Databases only) values for a column must come from a set of discrete values 
      Value for a column must be set to Yes, No, or Not Applicable
   
   9. Foreign-key
      (Databases only) values for a column must be unique values coming from a column in another table
      In a U.S. taxpayer database, the State column must be a valid state or territory with the set of acceptable values defined in a separate States table
   
   10. Accuracy
      The degree to which the data conforms to the actual entity being measured or described
      If values for zip codes are validated by street location, the accuracy of the data goes up.
      
   11. Completeness
      The degree to which the data contains all desired components or measures
      If data for personal profiles required hair and eye color, and both are collected, the data is complete.
      
   12. Consistency
      The degree to which the data is repeatable from different points of entry or collection
      If a customer has the same address in the sales and repair databases, the data is consistent.

-> Clean data + alignment to business objective = accurate conclusions

-> TYPES OF INSUFFICIENT DATA:
      1. Data from only one source
      2. Data that keeps updating
      3. Outdated data
      4. Geographically limited data

-> WAYS TO ADDRESS INSUFFICIENT DATA:
      1. Identify trends with the available data
      2. Wait for more data if time allows
      3. Talk with stakeholders and adjust your objective
      4. Look for a new dataset

-> A population is all possible data values in a certain dataset. 

-> SAMPLE SIZE: a part of a population that's representative of the population. The goal is to get enough information from a small group within a population to
   make predictions or conclusions about the whole population. The sample size helps ensure the degree to which you can be confident that your conclusions 
   accurately represent the population.

-> Sampling bias is when a sample isn't representative of the population as a whole. This means some members of the population are being overrepresented or 
   underrepresented. 
   
-> Random sampling is a way of selecting a sample from a population so that every possible type of the sample has an equal chance of being chosen. 

-> 1. Population 
      The entire group that you are interested in for your study. For example, if you are surveying people in your company, the population would be all the 
      employees in your company.
      
   2. Sample 
      A subset of your population. Just like a food sample, it is called a sample because it is only a taste. So if your company is too large to survey every 
      individual, you can survey a representative sample of your population.
   
   3. Margin of error
      Since a sample is used to represent a population, the sample’s results are expected to differ from what the result would have been if you had surveyed the
      entire population. This difference is called the margin of error. The smaller the margin of error, the closer the results of the sample are to what the
      result would have been if you had surveyed the entire population. 
    
   4. Confidence level
      How confident you are in the survey results. For example, a 95% confidence level means that if you were to run the same survey 100 times, you would get 
      similar results 95 of those 100 times. Confidence level is targeted before you start your study because it will affect how big your margin of error is at 
      the end of your study. 
   
   5. Confidence interval
      The range of possible values that the population’s result would be at the confidence level of the study. This range is the sample result +/- the margin of
      error.
   
   6. Statistical significance
      The determination of whether your result could be due to random chance or not. The greater the significance, the less due to chance.

-> Increase the sample size to meet specific needs of your project:

      1. For a higher confidence level, use a larger sample size

      2. To decrease the margin of error, use a larger sample size

      3. For greater statistical significance, use a larger sample size

-> Statistical power is the probability of getting meaningful results from a test. 

-> Hypothesis testing is a way to see if a survey or experiment has meaningful results.

-> If a test is statistically significant, it means the results of the test are real and not an error caused by random chance.

-> Which statistical power is typically considered the minimum for statistical significance? 0.8 or 80%

-> Statistical power can be calculated and reported for a completed experiment to comment on the confidence one might have in the conclusions drawn from the 
   results of the study. It can also be used as a tool to estimate the number of observations or sample size required in order to detect an effect in an experiment.
  
-> The confidence level is the probability that your sample accurately reflects the greater population. You can think of it the same way as confidence in anything
   else. It's how strongly you feel that you can rely on something or someone. Having a 99 percent confidence level is ideal. But most industries hope for at least
   a 90 or 95 percent confidence level. 

-> A sample size calculator tells you how many people you need to interview (or things you need to test) to get results that represent the target population. Let’s review some terms you will come across when using a sample size calculator:

      1. Confidence level: The probability that your sample size accurately reflects the greater population.

      2. Margin of error: The maximum amount that the sample results are expected to differ from those of the actual population.

      3. Population: This is the total number you hope to pull your sample from.

      4. Sample: A part of a population that is representative of the population.

      5. Estimated response rate: If you are running a survey of individuals, this is the percentage of people you expect will complete your survey out of those
         who received the survey.    

-> Margin of error is the maximum that the sample results are expected to differ from those of the actual population.

-> To calculate margin of error, you need three things: population size, sample size, and confidence level.
