-> Most common cause of poor quality data is human error.

-> Dirty data is data that's incomplete, incorrect, or irrelevant to the problem you're trying to solve.
   When you work with dirty data, you can't be sure that your results are correct. In fact, you can pretty much bet they won't be. Earlier, you learned that data 
   integrity is critical to reliable data analytics results, and clean data helps you achieve data integrity.
   
-> Clean data is data that's complete, correct, and relevant to the problem you're trying to solve. When you work with clean data, you'll find that your projects go 
   much more smoothly.

-> Data engineers transform data into a useful format for analysis and give it a reliable infrastructure. This means they develop, maintain, and test databases, 
   data processors and related systems. 
   
   Data warehousing specialists develop processes and procedures to effectively store and organize data. They make sure that data is available, secure, and backed up 
   to prevent loss.

-> Null is an indication that a value does not exist in a data set. Note that it's not the same as a zero. In the case of a survey, a null would mean the customers 
   skipped that question. A zero would mean they provided zero as their response.

-> TYPES OF DIRTY DATA:
   1. Duplicate data - Any data record that shows up more than once.
   2. Outdated data - Any data that is old which should be replaced with newer and more accurate information.
   3. Incomplete data - Any data that is missing important fields.
   4. Incorrect/inaccurate data - Any data that is complete but inaccurate.
   5. Inconsistent data - Any data that uses different formats to represent the same thing.

-> Misspelling, spelling variations, mixed up letters, inconsistent punctuation and typos in general, happen when someone types in a piece of data incorrectly.

-> Field is a single piece of information from a row or column of a spreadsheet. Field length is a tool for determining how many characters can be keyed into a field,
   assigning a certain length to this fields in your spreadsheet is a great way to avoid errors.
   
-> Data validation is a tool for checking the accuracy and quality of data before adding or importing it. Data validation is a form of data cleaning.

-> Validity
   The concept of using data integrity principles to ensure measures conform to defined business rules or constraints.
   
   Accuracy
   The degree of conformity of a measure to a standard or a true value.
   
   Completeness
   The degree to which all required measures are known.
   
   Consistency
   The degree to which a set of measures is equivalent across systems.

-> Before removing unwanted data from the data set, make a copy of the data set and work with the copy.
   Follow procedure step by step:
      1. Remove the duplicates
      2. Remove irrelevant data(data you dont need)
      3. Remove extra spaces or blanks
      4. Fixing misspellings
      5. Inconsistent capitalization
      6. Incorrect punctuation and other typos
      7. Removing formatting
      
-> A merger, which is an agreement that unites two organizations into a single new one.

-> Data merging is the process of combining two or more datasets into a single dataset. 

-> In data analytics, compatibility describes how well two or more datasets are able to work together.

-> FOUR QUESTIONS NEED TO BE ASKED:
      1. Do I have all the data that I need?
      2. Does the data I need exist in these datasets?
      3. Does the data need to be cleaned or is it ready for me to use?
      4. Are the datasets cleaned to the same standard?

-> Common mistakes to avoid
   1. Not checking for spelling errors: Misspellings can be as simple as typing or input errors. Most of the time the wrong spelling or common grammatical errors 
      can be detected, but it gets harder with things like names or addresses. For example, if you are working with a spreadsheet table of customer data, you might 
      come across a customer named “John” whose name has been input incorrectly as “Jon” in some places. The spreadsheet’s spellcheck probably won’t flag this, so
      if you don’t double-check for spelling errors and catch this, your analysis will have mistakes in it. 

   2. Forgetting to document errors: Documenting your errors can be a big time saver, as it helps you avoid those errors in the future by showing you how you 
      resolved them. For example, you might find an error in a formula in your spreadsheet. You discover that some of the dates in one of your columns haven’t been
      formatted correctly. If you make a note of this fix, you can reference it the next time your formula is broken, and get a head start on troubleshooting. 
      Documenting your errors also helps you keep track of changes in your work, so that you can backtrack if a fix didn’t work. 

   3. Not checking for misfielded values: A misfielded value happens when the values are entered into the wrong field. These values might still be formatted 
      correctly, which makes them harder to catch if you aren’t careful. For example, you might have a dataset with columns for cities and countries. These are the
      same type of data, so they are easy to mix up. But if you were trying to find all of the instances of Spain in the country column, and Spain had mistakenly
      been entered into the city column, you would miss key data points. Making sure your data has been entered correctly is key to accurate, complete analysis. 

   4. Overlooking missing values: Missing values in your dataset can create errors and give you inaccurate conclusions. For example, if you were trying to get the 
      total number of sales from the last three months, but a week of transactions were missing, your calculations would be inaccurate.  As a best practice, try to
      keep your data as clean as possible by maintaining completeness and consistency.

   5. Only looking at a subset of the data: It is important to think about all of the relevant data when you are cleaning. This helps make sure you understand the
      whole story the data is telling, and that you are paying attention to all possible errors. For example, if you are working with data about bird migration 
      patterns from different sources, but you only clean one source, you might not realize that some of the data is being repeated. This will cause problems in
      your analysis later on. If you want to avoid common errors like duplicates, each field of your data requires equal attention.

   6. Losing track of business objectives: When you are cleaning data, you might make new and interesting discoveries about your dataset-- but you don’t want those 
      discoveries to distract you from the task at hand. For example, if you were working with weather data to find the average number of rainy days in your city,
      you might notice some interesting patterns about snowfall, too. That is really interesting, but it isn’t related to the question you are trying to answer 
      right now. Being curious is great! But try not to let it distract you from the task at hand.  

   7. Not fixing the source of the error: Fixing the error itself is important. But if that error is actually part of a bigger problem, you need to find the source
      of the issue. Otherwise, you will have to keep fixing that same error over and over again. For example, imagine you have a team spreadsheet that tracks 
      everyone’s progress. The table keeps breaking because different people are entering different values. You can keep fixing all of these problems one by one, 
      or you can set up your table to streamline data entry so everyone is on the same page. Addressing the source of the errors in your data will save you a lot of
      time in the long run. 

   8. Not analyzing the system prior to data cleaning: If we want to clean our data and avoid future errors, we need to understand the root cause of your dirty 
      data. Imagine you are an auto mechanic. You would find the cause of the problem before you started fixing the car, right? The same goes for data. First, 
      you figure out where the errors come from. Maybe it is from a data entry error, not setting up a spell check, lack of formats, or from duplicates. Then, once
      you understand where bad data comes from, you can control it and keep your data clean.

   9. Not backing up your data prior to data cleaning: It is always good to be proactive and create your data backup before you start your data clean-up. If your 
      program crashes, or if your changes cause a problem in your dataset, you can always go back to the saved version and restore it. The simple procedure of 
      backing up your data can save you hours of work-- and most importantly, a headache. 

   10. Not accounting for data cleaning in your deadlines/process: All good things take time, and that includes data cleaning. It is important to keep that in mind
      when going through your process and looking at your deadlines. When you set aside time for data cleaning, it helps you get a more accurate estimate for ETAs 
      for stakeholders, and can help you know when to request an adjusted ETA. 

-> Conditional formatting is a spreadsheet tool that changes how cells appear when values meet specific conditions.

-> "Remove duplicates" is a tool that automatically searches for and eliminates duplicate entries from a spreadsheet.

-> In data analytics, a text string is a group of characters within a cell, most often composed of letters. An important characteristic of a text string is its 
   length, which is the number of characters in it.

-> Split is a tool that divides a text string around the specified character and puts each fragment into a new and separate cell. Split is helpful when you have
   more than one piece of data in a cell and you want to separate them out. 

-> COUNTIF is a function that returns the number of cells that match a specified value. Basically, it counts the number of times a value appears in a range of cells. 

-> Syntax is a predetermined structure that includes all required information and its proper placement. 

   The syntax of a COUNTIF function should be like this: Equals COUNTIF, open parenthesis, range, comma, the specified value in quotation marks and a closed 
   parenthesis. It will show up like this.

-> LEN is a function that tells you the length of the text string by counting the number of characters it contains. This is useful when cleaning data if you have a
   certain piece of information in your spreadsheet that you know must contain a certain length.

-> LEFT is a function that gives you a set number of characters from the left side of a text string. RIGHT is a function that gives you a set number of characters
   from the right side of a text string. As a quick reminder, a text string is a group of characters within a cell, commonly composed of letters, numbers, or both.

-> MID is a function that gives you a segment from the middle of a text string. 

-> CONCATENATE, which is a function that joins together two or more text strings. The syntax is equals CONCATENATE, then an open parenthesis inside indicates each
   text string you want to join, separated by commas. Then finish the function with a closed parenthesis.

-> TRIM is a function that removes leading, trailing, and repeated spaces in data. Sometimes when you import data, your cells have extra spaces, which can get 
   in the way of your analysis.
   
-> A pivot table is a data summarization tool that is used in data processing.
   Pivot tables sort, reorganize, group, count, total or average data stored in the database. In data cleaning, pivot tables are used to give you a quick, 
   clutter- free view of your data. You can choose to look at the specific parts of the data set that you need to get a visual in the form of a pivot table.

-> VLOOKUP stands for vertical lookup. It's a function that searches for a certain value in a column to return a corresponding piece of information. When data 
   analysts look up information for a project, it's rare for all of the data they need to be in the same place. Usually, you'll have to search across multiple 
   sheets or even different databases.
   The syntax of the VLOOKUP is equals VLOOKUP, open parenthesis, then the data you want to look up. Next is a comma and where you want to look for that data.

-> Data mapping is the process of matching fields from one database to another. This is very important to the success of data migration, data integration, and lots
   of other data management activities.

-> A schema is a way of describing how something is organized. A primary key references a column in which each value is unique and a foreign key is a field within 
   a table that is a primary key in another table.
