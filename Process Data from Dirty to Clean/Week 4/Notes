-> Verification is a process to confirm that a data cleaning effort was well- executed and the resulting data is accurate and reliable. It involves rechecking your 
   clean dataset, doing some manual clean ups if needed, and taking a moment to sit back and really think about the original purpose of the project. That way, you 
   can be confident that the data you collected is credible and appropriate for your purposes. 
   
   Making sure your data is properly verified is so important because it allows you to double-check that the work you did to clean up your data was thorough and 
   accurate.

-> Open communication is a lifeline for any data analytics project. Reports are a super effective way to show your team that you're being 100 percent transparent
   about your data cleaning. Reporting is also a great opportunity to show stakeholders that you're accountable, build trust with your team, and make sure you're 
   all on the same page of important project details. 
   Different strategies for reporting, include, creating data- cleaning reports, documenting your cleaning process, and using something called the changelog. 
   
-> A changelog is a file containing a chronologically ordered list of modifications made to a project. It's usually organized by version and includes the date
  followed by a list of added, improved, and removed features. Changelogs are very useful for keeping track of how a dataset evolved over the course of a project. 
  They're also another great way to communicate and report on data to others. 

-> The first step in the verification process is going back to your original unclean data set and comparing it to what you have now. Review the dirty data and try
   to identify any common problems. 

-> Another key part of verification involves taking a big-picture view of your project. This is an opportunity to confirm you're actually focusing on the business 
   problem that you need to solve and the overall project goals and to make sure that your data is actually capable of solving that problem and achieving those goals.
   It's important to take the time to reset and focus on the big picture because projects can sometimes evolve or transform over time without us even realizing it.

-> Taking a big picture view of your project involves doing three things. First, consider the business problem you're trying to solve with the data.
   If you've lost sight of the problem, you have no way of knowing what data belongs in your analysis. Taking a problem-first approach to analytics is essential at 
   all stages of any project. You need to be certain that your data will actually make it possible to solve your business problem. 
   
   Second, you need to consider the goal of the project. It's not enough just to know that your company wants to analyze customer feedback about a product. 
   What you really need to know is that the goal of getting this feedback is to make improvements to that product. On top of that, you also need to know whether the
   data you've collected and cleaned will actually help your company achieve that goal. 
   
   And third, you need to consider whether your data is capable of solving the problem and meeting the project objectives. That means thinking about where the data 
   came from and testing your data collection and cleaning processes.

-> Find and replace. Find and replace is a tool that looks for a specified search term in a spreadsheet and allows you to replace it with something else.

-> The CASE statement goes through one or more conditions and returns a value as soon as a condition is met. 
   SELECT 
   customer_id,
   CASE
      WHEN first_name = 'Tnoy' THEN 'Tony'
      ELSE first_name
      END AS cleaned_name
   FROM
   customer_data.customer_name
 
 -> Correct the most common problems
   Make sure you identified the most common problems and corrected them, including:

   1. Sources of errors: Did you use the right tools and functions to find the source of the errors in your dataset?

   2. Null data: Did you search for NULLs using conditional formatting and filters?

   3. Misspelled words: Did you locate all misspellings?

   4. Mistyped numbers: Did you double-check that your numeric data has been entered correctly?

   5. Extra spaces and characters: Did you remove any extra spaces or characters using the TRIM function?

   6. Duplicates: Did you remove duplicates in spreadsheets using the Remove Duplicates function or DISTINCT in SQL?

   7. Mismatched data types: Did you check that numeric, date, and string data are typecast correctly?

   8. Messy (inconsistent) strings: Did you make sure that all of your strings are consistent and meaningful?

   9. Messy (inconsistent) date formats: Did you format the dates consistently throughout your dataset?

   10. Misleading variable labels (columns): Did you name your columns meaningfully?

   11. Truncated data: Did you check for truncated or missing data that needs correction?

   12. Business Logic: Did you check that the data makes sense given your knowledge of the business? 

-> Review the goal of your project
   Once you have finished these data cleaning tasks, it is a good idea to review the goal of your project and confirm that your data is still aligned with that 
   goal. This is a continuous process that you will do throughout your project-- but here are three steps you can keep in mind while thinking about this: 

   1. Confirm the business problem 

   2. Confirm the goal of the project

   3. Verify that data can solve the problem and is aligned to the goal 

-> This involves documentation which is the process of tracking changes, additions, deletions and errors involved in your data cleaning effort.

-> Data errors are the crime, data cleaning is gathering evidence, and documentation is detailing exactly what happened for peer review or court. Having a record of
   how a data set evolved does three very important things. 
   First, it lets us recover data-cleaning errors. Instead of scratching our heads, trying to remember what we might have done three months ago, we have a cheat
   sheet to rely on if we come across the same errors again later. It's also a good idea to create a clean table rather than overriding your existing table.
   This way, you still have the original data in case you need to redo the cleaning. 
   Second, documentation gives you a way to inform other users of changes you've made. If you ever go on vacation or get promoted, the analyst who takes over for
   you will have a reference sheet to check in with. 
   Third, documentation helps you to determine the quality of the data to be used in analysis. The first two benefits assume the errors aren't fixable. 
   But if they are, a record gives the data engineer more information to refer to. It's also a great warning for ourselves that the data set is full of errors and 
   should be avoided in the future. If the errors were time-consuming to fix, it might be better to check out alternative data sets that we can use instead. 
   Data analysts usually use a changelog to access this information.

-> We can use Sheet's version history, which provides a real-time tracker of all the changes and who made them from individual cells to the entire worksheet. To
   find this feature, click the File tab, and then select Version history.
   In the right panel, choose an earlier version.
   We can find who edited the file and the changes they made in the column next to their name.
   To return to the current version, go to the top left and click "Back." If you want to check out changes in a specific cell, we can right-click and select Show
   Edit History.
   Also, if you want others to be able to browse a sheet's version history, you'll need to assign permission.
